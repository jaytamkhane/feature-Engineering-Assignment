{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf8d8bb4-32ff-4dc0-b997-146ad5e572b3",
   "metadata": {},
   "source": [
    "Theory Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837df24c-9e4b-497d-8b54-c3394f18c12c",
   "metadata": {},
   "source": [
    "1) What is a parameter?\n",
    ">  A parameter in machine learning (ML) is a variable that a model learns from data to make predictions. Parameters are internal to the model and are used to control how it behaves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2946e8b7-9c81-4081-ba87-a466ec6f283d",
   "metadata": {},
   "source": [
    "2) What is correlation? What does negative correlation mean?\n",
    ">  Correlation is a statistical measure that shows how two or more variables are related. It can be used to describe the strength and direction of the relationship between variables. A negative correlation is a relationship between two variables that move in opposite directions. It's also known as an inverse correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3baae-59d0-40bd-818c-3f8d9544cf4f",
   "metadata": {},
   "source": [
    "3) Define Machine Learning. What are the main components in Machine Learning?\n",
    ">  Machine learning (ML) is a type of AI that allows computers to learn and improve their performance through experience. The main components of ML are algorithms, data, models, and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0794ce-8a9d-4700-9891-b98c646dec4e",
   "metadata": {},
   "source": [
    "4) How does loss value help in determining whether the model is good or not?\n",
    ">  A lower loss value indicates a better performing model because it signifies that the model's predictions are closer to the actual target values, meaning it makes fewer errors. conversely, a high loss value means the model is making significant mistakes and is not performing well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10408018-fee4-40da-9001-70942f0d708e",
   "metadata": {},
   "source": [
    "5) What are continuous and categorical variables?\n",
    ">  A continuous variable can have any value between a theoretical minimum and maximum, like birth weight, BMI, temperature, neutrophil count,etc. while a categorical variable come in only a fixed number of values â€“ like dead/alive, obese/overweight/normal/underweight, pass/fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046dd7c8-350b-42b0-b805-b1c89aec5514",
   "metadata": {},
   "source": [
    "6) How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    ">  To handle categorical variables in machine learning, the most common technique is to convert them into numerical values using encoding methods like one-hot encoding, label encoding or ordinal encoding, depending on whether the categories have an inherent order or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2de95-6d09-455b-93b0-1deb360b23ce",
   "metadata": {},
   "source": [
    "7) What do you mean by training and testing a dataset?\n",
    "> The training dataset trains the machine learning model, allowing it to learn the patterns and relationships within the data. The test dataset used after the model has been trained and validated, to provide an unbiased evaluation of the model performance on completely new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d86fbb-726e-4f07-a2bc-9453137c9dc0",
   "metadata": {},
   "source": [
    "8) What is sklearn.preprocessing?\n",
    ">  The \"sklearn.preprocessing\" is a module in the \"scikit-learn\" library that provides functions and classes to transform raw data into a format suitable for machine learning models. This module includes techniques for scaling, normalizing, encoding, and imputing data, addressing common issues such as varying scales, non-normal distributions, categorical variables, and missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c17421-1288-4225-abf1-c6700fc46675",
   "metadata": {},
   "source": [
    "9) What is a Test set?\n",
    ">  A test set is a collection of tests that are used to evaluate a model or assess the performance of a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4e4f71-49f6-4390-9bfb-1b8d98fc2900",
   "metadata": {},
   "source": [
    "10) How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
    ">   To split data for model fitting in Python, we typically use the \"train_test_split\" function from the scikit-learn library, which divides your dataset into a training set (used to train the model) and a testing set (used to evaluate the model's performance on unseen data); a common approach is to split the data with an 80/20 ratio, where 80% becomes the training set and 20% the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26506627-187f-4256-aed4-8eb14ba14f34",
   "metadata": {},
   "source": [
    "11) Why do we have to perform EDA before fitting a model to the data?\n",
    ">   EDA allows you to gain a deep understanding of your data's structure, identify potential issues like outliers, missing values, or inconsistencies, and ultimately make informed decisions about feature selection, data cleaning, and model selection, leading to a more accurate and reliable model when fitted to the data, thats why EDA is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c822cc-d6f4-4ab7-963d-c42fdf6f21f9",
   "metadata": {},
   "source": [
    "12) What is correlation?\n",
    ">   In machine learning, correlation is a statistical analysis that measures how strongly two variables are related. It's a key part of data exploration and model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53defa5-047c-48f4-adec-31df10bcfb62",
   "metadata": {},
   "source": [
    "13) What does negative correlation mean?\n",
    ">   A negative correlation is also known as an inverse correlation. Two variables can have varying strengths of negative correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a921620-3d7f-4a05-a1b4-d952778eeb62",
   "metadata": {},
   "source": [
    "14) How can you find correlation between variables in Python?\n",
    ">   To calculate correlation in Python, you can use the pandas library, specifically the corr() method for DataFrames. It returns a correlation matrix showing the relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f8244-901c-41d3-adbf-e4450045efca",
   "metadata": {},
   "source": [
    "15) What is causation? Explain difference between correlation and causation with an example.\n",
    ">   Causation means that one event directly causes another event to happen, establishing a clear cause-and-effect relationship, while correlation simply indicates that two variables change together, not necessarily meaning one causes the other; essentially, correlation does not imply causation. \n",
    "Example\n",
    "Correlation: Ice cream sales and drowning incidents both tend to increase during the summer months, indicating a correlation between the two. \n",
    "Causation: Turning on a light switch directly causes the light to turn on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe9e84-0ab9-4a38-b744-adcba92ec885",
   "metadata": {},
   "source": [
    "16) What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    ">   An optimizer is an algorithm used in machine learning and deep learning to adjust the parameters (weights and biases) of a model to minimize the loss function and improve accuracy. It helps in faster and efficient convergence during training.\n",
    "a) Gradient Descent-Based Optimizers: These optimizers use gradient descent to minimize the loss function by updating parameters in the direction of the steepest descent. EX: (I) Batch Gradient Descent (BGD): Updates the parameters after computing the gradient of the entire dataset. (II) Stochastic Gradient Descent (SGD): Updates the parameters for each individual data point (sample). (III) Mini-Batch Gradient Descent (MBGD): Uses a small batch of data instead of a single data point or the entire dataset.\n",
    "b) Adaptive Optimizers: These optimizers improve upon gradient descent by adapting learning rates dynamically. EX: (I) Momentum Optimizer: Uses past gradients to smoothen updates. (II) RMSprop (Root Mean Square Propagation): Adjusts the learning rate based on recent gradient magnitudes.(III) Adam (Adaptive Moment Estimation): Combines Momentum and RMSprop for better optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d129fcb4-eb80-4421-8fec-0a0ebede312b",
   "metadata": {},
   "source": [
    "17) What is sklearn.linear_model ?\n",
    ">   The \"sklearn.linear_model\" is a module in the \"scikit-learn\" library that implements various linear models for regression, classification, and other tasks. Linear models find a linear relationship between the input features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb286d-7dd7-49bf-a766-1bf48c8caba3",
   "metadata": {},
   "source": [
    "18) What does model.fit() do? What arguments must be given?\n",
    ">   In machine learning, \"model.fit()\" is a method used to train a model by adjusting its internal parameters based on provided training data, essentially allowing the model to learn patterns and relationships within the data, it's the core function to initiate the training process in most machine learning libraries like scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc9e2a-0a5e-4eaf-9496-e8f1c2d38cec",
   "metadata": {},
   "source": [
    "19) What does model.predict() do? What arguments must be given?\n",
    ">   In machine learning, \"model.predict()\" is a function used to make predictions on new data using a trained model, it takes as its only argument the new data you want to predict labels for, and returns the predicted labels based on the patterns learned during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6725bf57-5af3-41ab-89c4-db6026c0e736",
   "metadata": {},
   "source": [
    "20) What are continuous and categorical variables?\n",
    ">   A continuous variable can take any value within a range, while a categorical variable can only take on a set of distinct values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630fee6-fb1f-49ad-b266-dcbe0880b1b5",
   "metadata": {},
   "source": [
    "21) What is feature scaling? How does it help in Machine Learning?\n",
    ">   Feature scaling is a data preprocessing step that rescales numerical features in a dataset to a common scale. It's also known as data normalization or Z-score normalization. (a) Improves model performance: Feature scaling helps ensure that all features contribute equally to the model, which can improve its performance. (b)Speeds up training: Feature scaling can speed up the convergence of gradient descent-based algorithms. (c)Improves separability of classes: Scaling features before reducing dimensionality can improve the separability of classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975abcc-663f-465e-bde7-4ab7391835ca",
   "metadata": {},
   "source": [
    "22) How do we perform scaling in Python?\n",
    ">   Scaling data in Python, particularly within machine learning, involves transforming numerical features to a standard range. This prevents features with larger values from dominating those with smaller values, ensuring each contributes equally to the analysis. Several methods can be employed using the scikit-learn library: (a) Min-Max Scaling (Normalization): This method scales data to a range between 0 and 1. (b) Standardization: It scales data to have a mean of 0 and a standard deviation of 1. (c) Robust Scaling: This method uses the median and interquartile range (IQR) to scale data, making it robust to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecab03d3-0b1d-4f72-a22d-d032f394c2b4",
   "metadata": {},
   "source": [
    "23) What is sklearn.preprocessing?\n",
    ">   The \"sklearn.preprocessing\" is a module in the \"scikit-learn\" library that provides functions and classes to transform raw data into a format suitable for machine learning models. This module includes techniques for scaling, normalizing, encoding, and imputing data, addressing common issues such as varying scales, non-normal distributions, categorical variables, and missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6f21d3-7ce6-43e3-b3b8-bfb193de2105",
   "metadata": {},
   "source": [
    "24) How do we split data for model fitting (training and testing) in Python?\n",
    ">   To split data for model fitting in Python, we typically use the \"train_test_split\" function from the scikit-learn library, which divides your dataset into a training set (used to train the model) and a testing set (used to evaluate the model's performance on unseen data); a common approach is to split the data with an 80/20 ratio, where 80% becomes the training set and 20% the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d9a18c-98f6-4cda-b41b-31b056b68477",
   "metadata": {},
   "source": [
    "25) Explain data encoding?\n",
    ">   The process of converting categorical data (textual or non-numerical) into a numerical format that can be understood and processed by machine learning algorithms is called Data Encoding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
